{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiTask Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "model_name = 'intfloat/e5-large-v2'\n",
    "vagueness_class_mapping = {\"specific\":1, \"ambiguous\":2, \"generic\":3, \"notESG\":0 }\n",
    "scope3_class_mapping = {\"yes\":1, \"no\":0}\n",
    "train_file_path = 'train_data.csv'\n",
    "test_file_path = 'test_data.csv'\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 8\n",
    "lr = 1e-5\n",
    "epochs = 30\n",
    "train_test_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format(sentences, max_sentence_length=200):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, use_fast=False)\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = max_sentence_length,\n",
    "                            padding = 'max_length',\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, scope3_class_mapping, vagueness_class_mapping, augment=False):\n",
    "        self.df = df\n",
    "        self.texts = self.df['text'].tolist()\n",
    "        self.scope3 = self.df['scope3'].apply(lambda x: scope3_class_mapping[x]).tolist()\n",
    "        self.vagueness = self.df['vague'].apply(lambda x: vagueness_class_mapping[x]).tolist()\n",
    "\n",
    "        if augment:\n",
    "\n",
    "            batch_size = 16\n",
    "            \n",
    "            aug1 = naw.BackTranslationAug(from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en', device='cuda', batch_size=batch_size, verbose=True)\n",
    "            aug2 = naw.SynonymAug(aug_src='wordnet')\n",
    "            \n",
    "            aug_text1 = aug1.augment(self.texts)\n",
    "            aug_text2 = aug2.augment(self.texts)\n",
    "\n",
    "            self.texts = np.concatenate([self.texts, aug_text1, aug_text2])\n",
    "            self.scope3 = np.concatenate([self.scope3, self.scope3, self.scope3])\n",
    "            self.vagueness = np.concatenate([self.vagueness, self.vagueness, self.vagueness])\n",
    "\n",
    "\n",
    "        self.input_ids, self.attention_masks = tokenize_and_format(self.texts)\n",
    "        self.scope3 = torch.tensor(self.scope3)\n",
    "        self.vagueness = torch.tensor(self.vagueness)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.vagueness[idx], self.scope3[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "train_len = len(train_df)\n",
    "test_len = len(test_df)\n",
    "\n",
    "num_val = int(train_test_split * train_len)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validation_df = train_df.iloc[:num_val]\n",
    "train_df = train_df.iloc[num_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8776, 975, 1083)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(validation_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set statistics\n",
      "scope3\n",
      "no     8272\n",
      "yes     504\n",
      "Name: count, dtype: int64\n",
      "vague\n",
      "notESG       4151\n",
      "specific     1858\n",
      "ambiguous    1590\n",
      "generic      1177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set statistics\n",
      "scope3\n",
      "no     910\n",
      "yes     65\n",
      "Name: count, dtype: int64\n",
      "vague\n",
      "notESG       432\n",
      "specific     205\n",
      "ambiguous    191\n",
      "generic      147\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set statistics\n",
      "scope3\n",
      "no     1024\n",
      "yes      59\n",
      "Name: count, dtype: int64\n",
      "vague\n",
      "notESG       490\n",
      "specific     222\n",
      "ambiguous    222\n",
      "generic      149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set statistics\")\n",
    "print(train_df['scope3'].value_counts())\n",
    "print(train_df['vague'].value_counts())\n",
    "\n",
    "print(\"\\nValidation set statistics\")\n",
    "print(validation_df['scope3'].value_counts())\n",
    "print(validation_df['vague'].value_counts())\n",
    "\n",
    "print(\"\\nTest set statistics\")\n",
    "print(test_df['scope3'].value_counts())\n",
    "print(test_df['vague'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMultiTaskDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope3_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvagueness_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m MultiTaskDataset(validation_df, scope3_class_mapping, vagueness_class_mapping)\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MultiTaskDataset(test_df, scope3_class_mapping, vagueness_class_mapping)\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mMultiTaskDataset.__init__\u001b[0;34m(self, df, scope3_class_mapping, vagueness_class_mapping, augment)\u001b[0m\n\u001b[1;32m     12\u001b[0m aug1 \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mBackTranslationAug(from_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wmt19-en-de\u001b[39m\u001b[38;5;124m'\u001b[39m, to_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/wmt19-de-en\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m aug2 \u001b[38;5;241m=\u001b[39m naw\u001b[38;5;241m.\u001b[39mSynonymAug(aug_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m aug_text1 \u001b[38;5;241m=\u001b[39m \u001b[43maug1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m aug_text2 \u001b[38;5;241m=\u001b[39m aug2\u001b[38;5;241m.\u001b[39maugment(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts, aug_text1, aug_text2])\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/nlpaug/base_augmenter.py:98\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstSummAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackTranslationAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsAug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextualWordEmbsForSentenceAug\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(aug_num):\n\u001b[0;32m---> 98\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    100\u001b[0m             augmented_results\u001b[38;5;241m.\u001b[39mextend(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/nlpaug/augmenter/word/back_translation.py:70\u001b[0m, in \u001b[0;36mBackTranslationAug.substitute\u001b[0;34m(self, data, n)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m---> 70\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_text\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/nlpaug/model/lang_models/machine_translation_transformers.py:40\u001b[0m, in \u001b[0;36mMtTransformers.predict\u001b[0;34m(self, texts, target_words, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, target_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     src_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_one_step_batched(texts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_tokenizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_model)\n\u001b[0;32m---> 40\u001b[0m     tgt_translated_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_one_step_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_translated_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgt_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tgt_translated_texts\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/nlpaug/model/lang_models/machine_translation_transformers.py:62\u001b[0m, in \u001b[0;36mMtTransformers.translate_one_step_batched\u001b[0;34m(self, data, tokenizer, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch)\n\u001b[1;32m     60\u001b[0m         input_ids, attention_mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 62\u001b[0m         translated_ids_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         all_translated_ids\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     68\u001b[0m             translated_ids_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     71\u001b[0m all_translated_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/utils.py:1655\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1649\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1650\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1651\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1652\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1653\u001b[0m     )\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1655\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/generation/utils.py:3171\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3168\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[1;32m   3170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3179\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/fsmt/modeling_fsmt.py:1221\u001b[0m, in \u001b[0;36mFSMTForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1219\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1221\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1240\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/fsmt/modeling_fsmt.py:1127\u001b[0m, in \u001b[0;36mFSMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, use_cache, output_attentions, output_hidden_states, inputs_embeds, decoder_inputs_embeds, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1121\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1122\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1123\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1124\u001b[0m     )\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1127\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_causal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/fsmt/modeling_fsmt.py:799\u001b[0m, in \u001b[0;36mFSMTDecoder.forward\u001b[0;34m(self, input_ids, encoder_hidden_states, encoder_padding_mask, decoder_padding_mask, decoder_causal_mask, head_mask, inputs_embeds, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    797\u001b[0m layer_state \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m x, layer_self_attn, layer_past, layer_cross_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_causal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[1;32m    812\u001b[0m     next_decoder_cache\u001b[38;5;241m.\u001b[39mappend(layer_past\u001b[38;5;241m.\u001b[39mcopy())\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/fsmt/modeling_fsmt.py:645\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, encoder_hidden_states, encoder_attn_mask, layer_state, causal_mask, layer_head_mask, cross_attn_layer_head_mask, decoder_padding_mask, output_attentions)\u001b[0m\n\u001b[1;32m    643\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    644\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m--> 645\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    648\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1514\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1514\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = MultiTaskDataset(train_df, scope3_class_mapping, vagueness_class_mapping, augment=True)\n",
    "valid_dataset = MultiTaskDataset(validation_df, scope3_class_mapping, vagueness_class_mapping)\n",
    "test_dataset = MultiTaskDataset(test_df, scope3_class_mapping, vagueness_class_mapping)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26328"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMultiTask(torch.nn.Module):\n",
    "    def __init__(self, encoder_model='bert-base-uncased'):\n",
    "        super(BERTMultiTask, self).__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.linear1 = torch.nn.Linear(hidden_size, 256)\n",
    "        self.scope3_out = torch.nn.Linear(256, 2)\n",
    "        self.vagueness_out = torch.nn.Linear(256, 4)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, mask):\n",
    "        outputs = self.encoder(input_ids, attention_mask=mask)\n",
    "\n",
    "        linear1_out = self.relu(self.linear1(outputs.last_hidden_state[:,0,:]))\n",
    "        scope3_out = self.scope3_out(linear1_out)\n",
    "        vagueness_out = self.vagueness_out(linear1_out)\n",
    "        return vagueness_out, scope3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTMultiTask(encoder_model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTMultiTask(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (scope3_out): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (vagueness_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 335.40583M\n"
     ]
    }
   ],
   "source": [
    "model_param_size = sum([p.nelement() for p in model.parameters()])\n",
    "print(f\"Model parameters: {model_param_size/1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_params = list(model.encoder.named_parameters())\n",
    "new_layer_params = list(model.scope3_out.named_parameters()) + list(model.vagueness_out.named_parameters()) + list(model.linear1.named_parameters())\n",
    "no_decay = {'bias', 'LayerNorm.weight'}\n",
    "\n",
    "base_learning_rate = 5e-7\n",
    "new_learning_rate = 1e-5\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in encoder_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01, 'lr': base_learning_rate},\n",
    "    {'params': [p for n, p in encoder_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': base_learning_rate},\n",
    "    {'params': [p for n, p in new_layer_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01, 'lr': new_learning_rate},\n",
    "    {'params': [p for n, p in new_layer_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': new_learning_rate}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=base_learning_rate, eps=1e-8)\n",
    "# optimizer = AdamW(model.parameters(), lr=base_learning_rate, eps=1e-8)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.33, patience=2, verbose=True)\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_true, class_mapping):\n",
    "    \n",
    "    y_pred_class = torch.argmax(y_pred, dim=-1)\n",
    "    reverse_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "\n",
    "    metrics = []\n",
    "    for i in reverse_class_mapping:\n",
    "\n",
    "        true_positives = torch.sum((y_pred_class == i) & (y_true == i)).item()\n",
    "        true_negatives = torch.sum((y_pred_class != i) & (y_true != i)).item()\n",
    "        false_positives = torch.sum((y_pred_class == i) & (y_true != i)).item()\n",
    "        false_negatives = torch.sum((y_pred_class != i) & (y_true == i)).item()\n",
    "\n",
    "        accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "\n",
    "        class_name = reverse_class_mapping[i]\n",
    "        metrics.append(f'Accuracy_{class_name}: {accuracy:.4f} | Precision_{class_name}: {precision:.4f} | Recall_{class_name}: {recall:.4f}|')\n",
    "\n",
    "    metrics = \" \".join(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 10000\n",
    "best_val_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 30 ========\n",
      "Batch [3291/3291], Average Train Loss: 1.2059 Accuracy_specific: 0.7999 | Precision_specific: 0.5440 | Recall_specific: 0.3384| Accuracy_ambiguous: 0.8145 | Precision_ambiguous: 0.4788 | Recall_ambiguous: 0.2671| Accuracy_generic: 0.8640 | Precision_generic: 0.4708 | Recall_generic: 0.1141| Accuracy_notESG: 0.6679 | Precision_notESG: 0.5959 | Recall_notESG: 0.9256| Accuracy_yes: 0.9434 | Precision_yes: 0.6842 | Recall_yes: 0.0258| Accuracy_no: 0.9434 | Precision_no: 0.9439 | Recall_no: 0.9993|\n",
      "Avg Validation Loss: 1.0805 | Best Validation Loss: 1.0805 | Best Epoch: 0 Accuracy_specific: 0.8297 | Precision_specific: 0.5990 | Recall_specific: 0.5756| Accuracy_ambiguous: 0.8082 | Precision_ambiguous: 0.5118 | Recall_ambiguous: 0.4555| Accuracy_generic: 0.8472 | Precision_generic: 0.4912 | Recall_generic: 0.3810| Accuracy_notESG: 0.7805 | Precision_notESG: 0.7206 | Recall_notESG: 0.8241| Accuracy_yes: 0.9344 | Precision_yes: 0.5128 | Recall_yes: 0.3077| Accuracy_no: 0.9344 | Precision_no: 0.9519 | Recall_no: 0.9791|\n",
      "======== Epoch 2 / 30 ========\n",
      "Batch [3291/3291], Average Train Loss: 0.9508 Accuracy_specific: 0.8290 | Precision_specific: 0.6078 | Recall_specific: 0.5429| Accuracy_ambiguous: 0.8354 | Precision_ambiguous: 0.5514 | Recall_ambiguous: 0.4916| Accuracy_generic: 0.8722 | Precision_generic: 0.5375 | Recall_generic: 0.3367| Accuracy_notESG: 0.7927 | Precision_notESG: 0.7350 | Recall_notESG: 0.8784| Accuracy_yes: 0.9553 | Precision_yes: 0.7958 | Recall_yes: 0.2989| Accuracy_no: 0.9553 | Precision_no: 0.9589 | Recall_no: 0.9953|\n",
      "Avg Validation Loss: 1.0528 | Best Validation Loss: 1.0528 | Best Epoch: 1 Accuracy_specific: 0.8410 | Precision_specific: 0.6126 | Recall_specific: 0.6634| Accuracy_ambiguous: 0.8185 | Precision_ambiguous: 0.5376 | Recall_ambiguous: 0.5236| Accuracy_generic: 0.8533 | Precision_generic: 0.5204 | Recall_generic: 0.3469| Accuracy_notESG: 0.7979 | Precision_notESG: 0.7505 | Recall_notESG: 0.8148| Accuracy_yes: 0.9272 | Precision_yes: 0.4500 | Recall_yes: 0.4154| Accuracy_no: 0.9272 | Precision_no: 0.9585 | Recall_no: 0.9637|\n",
      "======== Epoch 3 / 30 ========\n",
      "Batch [2357/3291], Average Train Loss: 0.8787 Accuracy_specific: 0.8397 | Precision_specific: 0.6395 | Recall_specific: 0.5829| Accuracy_ambiguous: 0.8486 | Precision_ambiguous: 0.5852 | Recall_ambiguous: 0.5455| Accuracy_generic: 0.8774 | Precision_generic: 0.5626 | Recall_generic: 0.3839| Accuracy_notESG: 0.8107 | Precision_notESG: 0.7585 | Recall_notESG: 0.8774| Accuracy_yes: 0.9610 | Precision_yes: 0.7854 | Recall_yes: 0.4448| Accuracy_no: 0.9610 | Precision_no: 0.9669 | Recall_no: 0.9926|"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m vagueness_loss \u001b[38;5;241m+\u001b[39m scope3_loss\n\u001b[1;32m     32\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m final_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mfinal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m running_vagueness_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([running_vagueness_tensor, vagueness_tensors])\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_vagueness_tensor = torch.tensor([]).to(device)\n",
    "    running_scope3_tensor = torch.tensor([]).to(device)\n",
    "    running_vagueness_pred_tensor = torch.tensor([]).to(device)\n",
    "    running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        \n",
    "        input_id_tensors = data[0].to(device)\n",
    "        input_mask_tensors = data[1].to(device)\n",
    "        vagueness_tensors = data[2].to(device)\n",
    "        scope3_tensors = data[3].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "        vagueness_loss = ce_loss(outputs[0], vagueness_tensors)\n",
    "        scope3_loss = ce_loss(outputs[1], scope3_tensors)\n",
    "\n",
    "        final_loss = vagueness_loss + scope3_loss\n",
    "\n",
    "        total_train_loss += final_loss.item()\n",
    "\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_vagueness_tensor = torch.cat([running_vagueness_tensor, vagueness_tensors])\n",
    "        running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "        running_vagueness_pred_tensor = torch.cat([running_vagueness_pred_tensor, outputs[0]])\n",
    "        running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs[1]])\n",
    "\n",
    "        vagueness_metrics = calculate_metrics(running_vagueness_pred_tensor, running_vagueness_tensor, vagueness_class_mapping)\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        average_train_loss = total_train_loss / (i+1)\n",
    "    \n",
    "        print(f'\\rBatch [{i+1}/{num_batches}], Average Train Loss: {average_train_loss:.4f}', vagueness_metrics, scope3_metrics, end='')\n",
    "\n",
    "    print(\"\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        total_valid_loss = 0\n",
    "        running_vagueness_tensor = torch.tensor([]).to(device)\n",
    "        running_scope3_tensor = torch.tensor([]).to(device)\n",
    "        running_vagueness_pred_tensor = torch.tensor([]).to(device)\n",
    "        running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "        for data in valid_dataloader:\n",
    "            \n",
    "            input_id_tensors = data[0].to(device)\n",
    "            input_mask_tensors = data[1].to(device)\n",
    "            vagueness_tensors = data[2].to(device)\n",
    "            scope3_tensors = data[3].to(device)\n",
    "\n",
    "            outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "            vagueness_loss = ce_loss(outputs[0], vagueness_tensors)\n",
    "            scope3_loss = ce_loss(outputs[1], scope3_tensors)\n",
    "\n",
    "            final_loss = vagueness_loss + scope3_loss\n",
    "            \n",
    "            total_valid_loss += final_loss.item()\n",
    "\n",
    "            running_vagueness_tensor = torch.cat([running_vagueness_tensor, vagueness_tensors])\n",
    "            running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "            running_vagueness_pred_tensor = torch.cat([running_vagueness_pred_tensor, outputs[0]])\n",
    "            running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs[1]])\n",
    "        \n",
    "        average_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "        scheduler.step(average_valid_loss)\n",
    "\n",
    "        if average_valid_loss < best_val_loss:\n",
    "            best_val_loss = average_valid_loss\n",
    "            best_val_epoch = epoch_i\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        vagueness_metrics = calculate_metrics(running_vagueness_pred_tensor, running_vagueness_tensor, vagueness_class_mapping)\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Avg Validation Loss: {average_valid_loss:.4f} | Best Validation Loss: {best_val_loss:.4f} | Best Epoch: {best_val_epoch}',vagueness_metrics, scope3_metrics)\n",
    "        \n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.texts = self.df['text'].values\n",
    "\n",
    "        self.input_ids, self.attention_masks = tokenize_and_format(self.texts)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BERTMultiTask(encoder_model=model_name)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "inference_dataset = InferenceDataset(test_df)\n",
    "inference_dataloader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# inference\n",
    "vagueness_inference = []\n",
    "scope3_inference = []\n",
    "\n",
    "reverse_vagueness_class_mapping = {v:k for k,v in vagueness_class_mapping.items()}\n",
    "reverse_scope3_class_mapping = {v:k for k,v in scope3_class_mapping.items()}\n",
    "with torch.no_grad():\n",
    "    for data in inference_dataloader:\n",
    "        \n",
    "        input_id_tensors = data[0].to(device)\n",
    "        input_mask_tensors = data[1].to(device)\n",
    "\n",
    "        outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "        vagueness_pred = torch.argmax(outputs[0], dim=-1).to('cpu').tolist()\n",
    "        scope3_pred = torch.argmax(outputs[1], dim=-1).to('cpu').tolist()\n",
    "\n",
    "        vagueness_inference.extend([reverse_vagueness_class_mapping[x] for x in vagueness_pred])\n",
    "        scope3_inference.extend([reverse_scope3_class_mapping[x] for x in scope3_pred])\n",
    "\n",
    "test_df['vagueness_pred'] = vagueness_inference\n",
    "test_df['scope3_pred'] = scope3_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>vagueness_pred</th>\n",
       "      <th>ambiguous</th>\n",
       "      <th>generic</th>\n",
       "      <th>notESG</th>\n",
       "      <th>specific</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vague</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ambiguous</th>\n",
       "      <td>108</td>\n",
       "      <td>22</td>\n",
       "      <td>47</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generic</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notESG</th>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>394</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specific</th>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "vagueness_pred  ambiguous  generic  notESG  specific\n",
       "vague                                               \n",
       "ambiguous             108       22      47        45\n",
       "generic                36       48      49        16\n",
       "notESG                 22       16     394        58\n",
       "specific               33        4      47       138"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.pivot_table(index='vague', columns='vagueness_pred', values='text', aggfunc='count', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>scope3_pred</th>\n",
       "      <th>no</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scope3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>995</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scope3_pred   no  yes\n",
       "scope3               \n",
       "no           995   29\n",
       "yes           27   32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.pivot_table(index='scope3', columns='scope3_pred', values='text', aggfunc='count', fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope 3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "model_name = 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "scope3_class_mapping = {\"yes\":1, \"no\":0}\n",
    "train_file_path = 'final_annotated_data.csv'\n",
    "test_file_path = 'NYSE_DE_2022_results.csv'\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 8\n",
    "lr = 1e-5\n",
    "epochs = 30\n",
    "train_test_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format(sentences, max_sentence_length=200):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, use_fast=False)\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = max_sentence_length,\n",
    "                            padding = 'max_length',\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, scope3_class_mapping):\n",
    "        self.df = df\n",
    "        self.texts = self.df['text'].values\n",
    "        self.scope3 = self.df['scope3'].apply(lambda x: scope3_class_mapping[x]).values\n",
    "\n",
    "        self.input_ids, self.attention_masks = tokenize_and_format(self.texts)\n",
    "        self.scope3 = torch.tensor(self.scope3)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.scope3[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "train_len = len(train_df)\n",
    "test_len = len(test_df)\n",
    "\n",
    "num_val = int(train_test_split * train_len)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validation_df = train_df.iloc[:num_val]\n",
    "train_df = train_df.iloc[num_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9751, 1083, 791)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(validation_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set statistics\n",
      "scope3\n",
      "no     9182\n",
      "yes     569\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set statistics\n",
      "scope3\n",
      "no     1024\n",
      "yes      59\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set statistics\n",
      "scope3\n",
      "no     762\n",
      "yes     29\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set statistics\")\n",
    "print(train_df['scope3'].value_counts())\n",
    "\n",
    "print(\"\\nValidation set statistics\")\n",
    "print(validation_df['scope3'].value_counts())\n",
    "\n",
    "print(\"\\nTest set statistics\")\n",
    "print(test_df['scope3'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b3073d34f342059366c72246bdba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa160f68a73942c9b21a938c3122d0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f104636cca74c5fb1c30f4607ce0691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a62cd0099a4580903ab5bd7b9b1751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = MultiTaskDataset(train_df, scope3_class_mapping)\n",
    "valid_dataset = MultiTaskDataset(validation_df, scope3_class_mapping)\n",
    "test_dataset = MultiTaskDataset(test_df, scope3_class_mapping)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMultiTask(torch.nn.Module):\n",
    "    def __init__(self, encoder_model='bert-base-uncased'):\n",
    "        super(BERTMultiTask, self).__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.linear1 = torch.nn.Linear(hidden_size, 256)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear2 = torch.nn.Linear(256, 256)\n",
    "        self.scope3_out = torch.nn.Linear(256, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, mask):\n",
    "        outputs = self.encoder(input_ids, attention_mask=mask)\n",
    "\n",
    "        linear1_out = self.relu(self.linear1(outputs.last_hidden_state[:,0,:]))\n",
    "        # linear1_out = self.dropout(linear1_out)\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        scope3_out = self.scope3_out(linear2_out)\n",
    "        return scope3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTMultiTask(encoder_model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTMultiTask(\n",
       "  (encoder): NewModel(\n",
       "    (embeddings): NewEmbeddings(\n",
       "      (word_embeddings): Embedding(30528, 1024, padding_idx=0)\n",
       "      (rotary_emb): NTKScalingRotaryEmbedding()\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): NewEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x NewLayer(\n",
       "          (attention): NewAttention(\n",
       "            (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): NewGatedMLP(\n",
       "            (up_gate_proj): Linear(in_features=1024, out_features=8192, bias=False)\n",
       "            (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (act_fn): GELUActivation()\n",
       "            (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (attn_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (mlp_ln): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (scope3_out): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 434.467842M\n"
     ]
    }
   ],
   "source": [
    "model_param_size = sum([p.nelement() for p in model.parameters()])\n",
    "print(f\"Model parameters: {model_param_size/1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_params = list(model.encoder.named_parameters())\n",
    "new_layer_params = list(model.scope3_out.named_parameters()) + list(model.linear1.named_parameters())\n",
    "no_decay = {'bias', 'LayerNorm.weight'}\n",
    "\n",
    "base_learning_rate = 5e-7\n",
    "new_learning_rate = 1e-4\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in encoder_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01, 'lr': base_learning_rate},\n",
    "    {'params': [p for n, p in encoder_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': base_learning_rate},\n",
    "    {'params': [p for n, p in new_layer_params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01, 'lr': new_learning_rate},\n",
    "    {'params': [p for n, p in new_layer_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, 'lr': new_learning_rate}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=base_learning_rate, eps=1e-8)\n",
    "optimizer = AdamW(model.parameters(), lr=new_learning_rate, eps=1e-6, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.33, patience=2, verbose=True)\n",
    "ce_loss = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 10.0]).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_true, class_mapping):\n",
    "    \n",
    "    y_pred_class = torch.argmax(y_pred, dim=-1)\n",
    "    reverse_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "\n",
    "    metrics = []\n",
    "    for i in reverse_class_mapping:\n",
    "\n",
    "        true_positives = torch.sum((y_pred_class == i) & (y_true == i)).item()\n",
    "        true_negatives = torch.sum((y_pred_class != i) & (y_true != i)).item()\n",
    "        false_positives = torch.sum((y_pred_class == i) & (y_true != i)).item()\n",
    "        false_negatives = torch.sum((y_pred_class != i) & (y_true == i)).item()\n",
    "\n",
    "        accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "\n",
    "        class_name = reverse_class_mapping[i]\n",
    "        metrics.append(f'Accuracy_{class_name}: {accuracy:.4f} | Precision_{class_name}: {precision:.4f} | Recall_{class_name}: {recall:.4f}|')\n",
    "\n",
    "    metrics = \" \".join(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 10000\n",
    "best_val_epoch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  2949,  1996,  ...,     0,     0,     0],\n",
       "         [  101,  3891,  2241,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  4254,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  1999,  2047,  ...,     0,     0,     0],\n",
       "         [  101, 18368,  2038,  ...,     0,     0,     0],\n",
       "         [  101,  4031,  3737,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([0, 0, 1, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0128, -0.2004],\n",
       "        [ 0.0771, -0.1036],\n",
       "        [ 0.0242, -0.1750],\n",
       "        [ 0.1064, -0.1178],\n",
       "        [ 0.0180, -0.1614],\n",
       "        [ 0.1002, -0.0969],\n",
       "        [ 0.1553, -0.1944],\n",
       "        [ 0.0689, -0.1395]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(train_dataloader))[0].to(device), next(iter(train_dataloader))[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[-0.2923, -0.1847],\n",
    "        [-0.1975, -0.1444],\n",
    "        [-0.1531, -0.1217],\n",
    "        [-0.2194, -0.1925],\n",
    "        [-0.1369, -0.1668],\n",
    "        [-0.2941, -0.3014],\n",
    "        [-0.1777, -0.1966],\n",
    "        [-0.2073, -0.1600]]).to(device)\n",
    "c = next(iter(train_dataloader))[2].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7066, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 30 ========\n",
      "Batch [1219/1219], Average Train Loss: 0.3206 Accuracy_yes: 0.9153 | Precision_yes: 0.3657 | Recall_yes: 0.6151| Accuracy_no: 0.9153 | Precision_no: 0.9751 | Recall_no: 0.9339|\n",
      "Avg Validation Loss: 0.2734 | Best Validation Loss: 0.2734 | Best Epoch: 0 Accuracy_yes: 0.9391 | Precision_yes: 0.4578 | Recall_yes: 0.6441| Accuracy_no: 0.9391 | Precision_no: 0.9790 | Recall_no: 0.9561|\n",
      "======== Epoch 2 / 30 ========\n",
      "Batch [1219/1219], Average Train Loss: 0.2468 Accuracy_yes: 0.9189 | Precision_yes: 0.3980 | Recall_yes: 0.7610| Accuracy_no: 0.9189 | Precision_no: 0.9843 | Recall_no: 0.9287|\n",
      "Avg Validation Loss: 0.3062 | Best Validation Loss: 0.2734 | Best Epoch: 0 Accuracy_yes: 0.9372 | Precision_yes: 0.4444 | Recall_yes: 0.6102| Accuracy_no: 0.9372 | Precision_no: 0.9770 | Recall_no: 0.9561|\n",
      "======== Epoch 3 / 30 ========\n",
      "Batch [1219/1219], Average Train Loss: 0.2178 Accuracy_yes: 0.9281 | Precision_yes: 0.4367 | Recall_yes: 0.7996| Accuracy_no: 0.9281 | Precision_no: 0.9869 | Recall_no: 0.9361|\n",
      "Avg Validation Loss: 0.2830 | Best Validation Loss: 0.2734 | Best Epoch: 0 Accuracy_yes: 0.9187 | Precision_yes: 0.3717 | Recall_yes: 0.7119| Accuracy_no: 0.9187 | Precision_no: 0.9825 | Recall_no: 0.9307|\n",
      "======== Epoch 4 / 30 ========\n",
      "Batch [58/1219], Average Train Loss: 0.1418 Accuracy_yes: 0.9591 | Precision_yes: 0.6216 | Recall_yes: 0.8214| Accuracy_no: 0.9591 | Precision_no: 0.9883 | Recall_no: 0.9679|"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m scope3_loss \u001b[38;5;241m=\u001b[39m ce_loss(outputs, scope3_tensors)\n\u001b[1;32m     26\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m scope3_loss\n\u001b[0;32m---> 28\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mfinal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m final_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_scope3_tensor = torch.tensor([]).to(device)\n",
    "    running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        \n",
    "        input_id_tensors = data[0].to(device)\n",
    "        input_mask_tensors = data[1].to(device)\n",
    "        scope3_tensors = data[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "        scope3_loss = ce_loss(outputs, scope3_tensors)\n",
    "\n",
    "        final_loss = scope3_loss\n",
    "\n",
    "        total_train_loss += final_loss.item()\n",
    "\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "        running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs])\n",
    "\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        average_train_loss = total_train_loss / (i+1)\n",
    "    \n",
    "        print(f'\\rBatch [{i+1}/{num_batches}], Average Train Loss: {average_train_loss:.4f}', scope3_metrics, end='')\n",
    "\n",
    "    print(\"\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        total_valid_loss = 0\n",
    "        running_scope3_tensor = torch.tensor([]).to(device)\n",
    "        running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "        for data in valid_dataloader:\n",
    "            \n",
    "            input_id_tensors = data[0].to(device)\n",
    "            input_mask_tensors = data[1].to(device)\n",
    "            scope3_tensors = data[2].to(device)\n",
    "\n",
    "            outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "            scope3_loss = ce_loss(outputs, scope3_tensors)\n",
    "\n",
    "            final_loss = scope3_loss\n",
    "            \n",
    "            total_valid_loss += final_loss.item()\n",
    "\n",
    "            running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "            running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs])\n",
    "        \n",
    "        average_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "        scheduler.step(average_valid_loss)\n",
    "\n",
    "        if average_valid_loss < best_val_loss:\n",
    "            best_val_loss = average_valid_loss\n",
    "            best_val_epoch = epoch_i\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Avg Validation Loss: {average_valid_loss:.4f} | Best Validation Loss: {best_val_loss:.4f} | Best Epoch: {best_val_epoch}',scope3_metrics)\n",
    "        \n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.texts = self.df['text'].values\n",
    "\n",
    "        self.input_ids, self.attention_masks = tokenize_and_format(self.texts)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BERTMultiTask(encoder_model=model_name)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# inference\n",
    "vagueness_inference = []\n",
    "scope3_inference = []\n",
    "\n",
    "reverse_scope3_class_mapping = {v:k for k,v in scope3_class_mapping.items()}\n",
    "with torch.no_grad():\n",
    "    for data in valid_dataloader:\n",
    "        \n",
    "        input_id_tensors = data[0].to(device)\n",
    "        input_mask_tensors = data[1].to(device)\n",
    "\n",
    "        outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "        scope3_pred = torch.argmax(outputs, dim=-1).to('cpu').tolist()\n",
    "\n",
    "        scope3_inference.extend([reverse_scope3_class_mapping[x] for x in scope3_pred])\n",
    "\n",
    "# result_df = pd.DataFrame({'text':inference_data, 'scope3':scope3_inference})\n",
    "# print(result_df)\n",
    "validation_df['prediction'] = scope3_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['text'] = validation_df['text'].apply(lambda x: x.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "validation_df.to_csv('validation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gpt_responses</th>\n",
       "      <th>scope3</th>\n",
       "      <th>vague</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>At NIKE, we have a long history of working tow...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'generic'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>generic</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SECTION SCOPE: In this section our climate emi...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Progress: We are partnering in the collection ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_MCD_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>The Companys goal is to provide its customers...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>How a company implements policies and procedur...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>These emissions are broken out into Purchased ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NASDAQ_BKNG_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Loss of riders and Uber Eats users: Consumer p...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_UBER_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Transparent processes and systems to help ensu...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Acquired 1,500 EV charge points in Singapore E...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Because approximately 85% of our total carbon ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>We choose third parties based, in part, on the...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>To encourage more sustainable fuel use in ther...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Scope 3 TotalEnergies reports Scope 3 GHG emis...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Product that is not eligible for Nike Refurbis...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Then, we plan to scale the framework to strate...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>NIKE does not have significant emissions from ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Rooftop solar projects, which are concentrated...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>It will allow industrial emitters in Norway an...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>We are on track to meet our goal of achieving ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_PFE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Smallholder farmers play an essential role in ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'generic'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>generic</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Currently 29% of our suppliers by spend have o...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_PFE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Scope 3 (commercial air travel) data used to r...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>The sharp rise in sales of electricity (a twen...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>In order to continue to be eligible for our Su...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_DE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>NIKE is also working to push suppliers in the ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Additionally, NIKE is exploring policy framewo...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Implement HRDD systems in certain targeted par...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Globally, we face a critical lack of scaled, i...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>The program is showing signs of success 10 sup...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Eleven manufacturing suppliers belonging to th...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>We have worked with material and chemical supp...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>In addition, we are including our priority sou...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>The NIKE Supplier Climate Action Program (SCAP...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>We also conducted a global risk assessment to ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_KO_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>This has resulted in the ability to leverage d...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Currently, we are estimating Scope 3, Category...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_DE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>Through this program we were able to offset al...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NASDAQ_BKNG_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Franchisees, but extrapolates where it does no...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_MCD_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Rides: Rides in electric taxis or public trans...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NASDAQ_BKNG_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>In 2021, we surpassed 5.5 billion in annual sp...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_XOM_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Before FY22, NIKE calculated EOL impacts of NI...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>We have verified through the ones undertaken a...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_MCD_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Tier 1 focus factory data is self reported by ...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>By contrast, it does not have control over emi...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>As well as designing our sites to be more sust...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_MCD_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Water consumed throughout NIKEs value chain m...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'generic'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>generic</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Excludes emissions from consumers traveling to...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>We spent 2.4 billion of the 5.5 billion in the...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_XOM_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>We work with suppliers to mutually set objecti...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_MCD_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>Further demonstrating our commitment to sustai...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_DE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>Air CO2 emissions are estimated based on numbe...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>We also accelerated our capabilities to keep F...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>NIKEs products are finished consumer goods an...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'notESG'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>notESG</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>When the time has come to say goodbye to a pai...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>In EMEA, we continued expanding the use of hyd...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Starting in 2023, environmental audits of supp...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_TTE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>When new materials and manufacturing processes...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>By the end of FY21, all in scope supplier coal...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'specific'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>specific</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>These emissions are associated with washing an...</td>\n",
       "      <td>{'scope3': 'yes', 'vague': 'ambiguous'}</td>\n",
       "      <td>yes</td>\n",
       "      <td>ambiguous</td>\n",
       "      <td>NYSE_NKE_2022_results.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "43    At NIKE, we have a long history of working tow...   \n",
       "52    SECTION SCOPE: In this section our climate emi...   \n",
       "98    Progress: We are partnering in the collection ...   \n",
       "111   The Companys goal is to provide its customers...   \n",
       "112   How a company implements policies and procedur...   \n",
       "125   These emissions are broken out into Purchased ...   \n",
       "137   Loss of riders and Uber Eats users: Consumer p...   \n",
       "160   Transparent processes and systems to help ensu...   \n",
       "167   Acquired 1,500 EV charge points in Singapore E...   \n",
       "176   Because approximately 85% of our total carbon ...   \n",
       "178   We choose third parties based, in part, on the...   \n",
       "205   To encourage more sustainable fuel use in ther...   \n",
       "211   Scope 3 TotalEnergies reports Scope 3 GHG emis...   \n",
       "224   Product that is not eligible for Nike Refurbis...   \n",
       "231   Then, we plan to scale the framework to strate...   \n",
       "257   NIKE does not have significant emissions from ...   \n",
       "283   Rooftop solar projects, which are concentrated...   \n",
       "288   It will allow industrial emitters in Norway an...   \n",
       "293   We are on track to meet our goal of achieving ...   \n",
       "317   Smallholder farmers play an essential role in ...   \n",
       "318   Currently 29% of our suppliers by spend have o...   \n",
       "339   Scope 3 (commercial air travel) data used to r...   \n",
       "345   The sharp rise in sales of electricity (a twen...   \n",
       "358   In order to continue to be eligible for our Su...   \n",
       "381   NIKE is also working to push suppliers in the ...   \n",
       "511   Additionally, NIKE is exploring policy framewo...   \n",
       "512   Implement HRDD systems in certain targeted par...   \n",
       "519   Globally, we face a critical lack of scaled, i...   \n",
       "531   The program is showing signs of success 10 sup...   \n",
       "620   Eleven manufacturing suppliers belonging to th...   \n",
       "636   We have worked with material and chemical supp...   \n",
       "648   In addition, we are including our priority sou...   \n",
       "652   The NIKE Supplier Climate Action Program (SCAP...   \n",
       "664   We also conducted a global risk assessment to ...   \n",
       "668   This has resulted in the ability to leverage d...   \n",
       "675   Currently, we are estimating Scope 3, Category...   \n",
       "684   Through this program we were able to offset al...   \n",
       "729   Franchisees, but extrapolates where it does no...   \n",
       "751   Rides: Rides in electric taxis or public trans...   \n",
       "760   In 2021, we surpassed 5.5 billion in annual sp...   \n",
       "774   Before FY22, NIKE calculated EOL impacts of NI...   \n",
       "775   We have verified through the ones undertaken a...   \n",
       "778   Tier 1 focus factory data is self reported by ...   \n",
       "812   By contrast, it does not have control over emi...   \n",
       "813   As well as designing our sites to be more sust...   \n",
       "826   Water consumed throughout NIKEs value chain m...   \n",
       "840   Excludes emissions from consumers traveling to...   \n",
       "870   We spent 2.4 billion of the 5.5 billion in the...   \n",
       "918   We work with suppliers to mutually set objecti...   \n",
       "929   Further demonstrating our commitment to sustai...   \n",
       "939   Air CO2 emissions are estimated based on numbe...   \n",
       "948   We also accelerated our capabilities to keep F...   \n",
       "989   NIKEs products are finished consumer goods an...   \n",
       "996   When the time has come to say goodbye to a pai...   \n",
       "1016  In EMEA, we continued expanding the use of hyd...   \n",
       "1032  Starting in 2023, environmental audits of supp...   \n",
       "1040  When new materials and manufacturing processes...   \n",
       "1041  By the end of FY21, all in scope supplier coal...   \n",
       "1070  These emissions are associated with washing an...   \n",
       "\n",
       "                                gpt_responses scope3      vague  \\\n",
       "43      {'scope3': 'yes', 'vague': 'generic'}    yes    generic   \n",
       "52    {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "98    {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "111    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "112   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "125   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "137   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "160    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "167   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "176    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "178   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "205   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "211    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "224    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "231   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "257    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "283    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "288   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "293   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "317     {'scope3': 'yes', 'vague': 'generic'}    yes    generic   \n",
       "318    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "339    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "345    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "358    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "381    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "511   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "512   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "519   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "531    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "620    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "636    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "648   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "652   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "664   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "668    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "675   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "684    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "729   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "751   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "760    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "774   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "775    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "778   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "812   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "813   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "826     {'scope3': 'yes', 'vague': 'generic'}    yes    generic   \n",
       "840    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "870    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "918   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "929   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "939    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "948    {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "989      {'scope3': 'yes', 'vague': 'notESG'}    yes     notESG   \n",
       "996   {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "1016   {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "1032  {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "1040  {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "1041   {'scope3': 'yes', 'vague': 'specific'}    yes   specific   \n",
       "1070  {'scope3': 'yes', 'vague': 'ambiguous'}    yes  ambiguous   \n",
       "\n",
       "                         file_name  \n",
       "43       NYSE_NKE_2022_results.csv  \n",
       "52        NYSE_KO_2022_results.csv  \n",
       "98       NYSE_MCD_2022_results.csv  \n",
       "111      NYSE_TTE_2022_results.csv  \n",
       "112       NYSE_KO_2022_results.csv  \n",
       "125   NASDAQ_BKNG_2022_results.csv  \n",
       "137     NYSE_UBER_2022_results.csv  \n",
       "160      NYSE_NKE_2022_results.csv  \n",
       "167      NYSE_TTE_2022_results.csv  \n",
       "176       NYSE_KO_2022_results.csv  \n",
       "178      NYSE_NKE_2022_results.csv  \n",
       "205      NYSE_NKE_2022_results.csv  \n",
       "211      NYSE_TTE_2022_results.csv  \n",
       "224      NYSE_NKE_2022_results.csv  \n",
       "231      NYSE_NKE_2022_results.csv  \n",
       "257      NYSE_NKE_2022_results.csv  \n",
       "283      NYSE_NKE_2022_results.csv  \n",
       "288      NYSE_TTE_2022_results.csv  \n",
       "293      NYSE_PFE_2022_results.csv  \n",
       "317       NYSE_KO_2022_results.csv  \n",
       "318      NYSE_PFE_2022_results.csv  \n",
       "339      NYSE_NKE_2022_results.csv  \n",
       "345      NYSE_TTE_2022_results.csv  \n",
       "358       NYSE_DE_2022_results.csv  \n",
       "381      NYSE_NKE_2022_results.csv  \n",
       "511      NYSE_NKE_2022_results.csv  \n",
       "512       NYSE_KO_2022_results.csv  \n",
       "519      NYSE_NKE_2022_results.csv  \n",
       "531       NYSE_KO_2022_results.csv  \n",
       "620      NYSE_NKE_2022_results.csv  \n",
       "636      NYSE_NKE_2022_results.csv  \n",
       "648       NYSE_KO_2022_results.csv  \n",
       "652      NYSE_NKE_2022_results.csv  \n",
       "664       NYSE_KO_2022_results.csv  \n",
       "668      NYSE_NKE_2022_results.csv  \n",
       "675       NYSE_DE_2022_results.csv  \n",
       "684   NASDAQ_BKNG_2022_results.csv  \n",
       "729      NYSE_MCD_2022_results.csv  \n",
       "751   NASDAQ_BKNG_2022_results.csv  \n",
       "760      NYSE_XOM_2022_results.csv  \n",
       "774      NYSE_NKE_2022_results.csv  \n",
       "775      NYSE_MCD_2022_results.csv  \n",
       "778      NYSE_NKE_2022_results.csv  \n",
       "812      NYSE_TTE_2022_results.csv  \n",
       "813      NYSE_MCD_2022_results.csv  \n",
       "826      NYSE_NKE_2022_results.csv  \n",
       "840      NYSE_NKE_2022_results.csv  \n",
       "870      NYSE_XOM_2022_results.csv  \n",
       "918      NYSE_MCD_2022_results.csv  \n",
       "929       NYSE_DE_2022_results.csv  \n",
       "939      NYSE_NKE_2022_results.csv  \n",
       "948      NYSE_NKE_2022_results.csv  \n",
       "989      NYSE_NKE_2022_results.csv  \n",
       "996      NYSE_NKE_2022_results.csv  \n",
       "1016     NYSE_NKE_2022_results.csv  \n",
       "1032     NYSE_TTE_2022_results.csv  \n",
       "1040     NYSE_NKE_2022_results.csv  \n",
       "1041     NYSE_NKE_2022_results.csv  \n",
       "1070     NYSE_NKE_2022_results.csv  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df[validation_df['scope3'] == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Variables\n",
    "model_name = 'intfloat/e5-base-v2'\n",
    "scope3_class_mapping = {\"yes\":1, \"no\":0}\n",
    "train_file_path = 'final_annotated_data.csv'\n",
    "test_file_path = 'NYSE_DE_2022_results.csv'\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 8\n",
    "lr = 1e-5\n",
    "epochs = 30\n",
    "train_test_split = 0.1\n",
    "\n",
    "def tokenize_and_format(sentences, max_sentence_length=200):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True, use_fast=False)\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = max_sentence_length,\n",
    "                            padding = 'max_length',\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, scope3_class_mapping):\n",
    "        self.df = df\n",
    "        self.texts = self.df['text'].values\n",
    "        self.scope3 = self.df['scope3'].apply(lambda x: scope3_class_mapping[x]).values\n",
    "\n",
    "        self.input_ids, self.attention_masks = tokenize_and_format(self.texts)\n",
    "        self.scope3 = torch.tensor(self.scope3)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.scope3[idx]\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "train_len = len(train_df)\n",
    "test_len = len(test_df)\n",
    "\n",
    "num_val = int(train_test_split * train_len)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validation_df = train_df.iloc[:num_val]\n",
    "train_df = train_df.iloc[num_val:]\n",
    "\n",
    "len(train_df), len(validation_df), len(test_df)\n",
    "print(\"Train set statistics\")\n",
    "print(train_df['scope3'].value_counts())\n",
    "\n",
    "print(\"\\nValidation set statistics\")\n",
    "print(validation_df['scope3'].value_counts())\n",
    "\n",
    "print(\"\\nTest set statistics\")\n",
    "print(test_df['scope3'].value_counts())\n",
    "\n",
    "train_dataset = MultiTaskDataset(train_df, scope3_class_mapping)\n",
    "valid_dataset = MultiTaskDataset(validation_df, scope3_class_mapping)\n",
    "test_dataset = MultiTaskDataset(test_df, scope3_class_mapping)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "class BERTMultiTask(torch.nn.Module):\n",
    "    def __init__(self, encoder_model='bert-base-uncased'):\n",
    "        super(BERTMultiTask, self).__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.linear1 = torch.nn.Linear(hidden_size, 256)\n",
    "        self.scope3_out = torch.nn.Linear(256, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, mask):\n",
    "        outputs = self.encoder(input_ids, attention_mask=mask)\n",
    "\n",
    "        linear1_out = self.relu(self.linear1(outputs.last_hidden_state[:,0,:]))\n",
    "        scope3_out = self.scope3_out(linear1_out)\n",
    "        return scope3_out\n",
    "model = BERTMultiTask(encoder_model=model_name)\n",
    "model.to(device)\n",
    "model_param_size = sum([p.nelement() for p in model.parameters()])\n",
    "print(f\"Model parameters: {model_param_size/1e6}M\")\n",
    "encoder_params = list(model.encoder.named_parameters())\n",
    "new_layer_params = list(model.scope3_out.named_parameters()) + list(model.linear1.named_parameters())\n",
    "no_decay = {'bias', 'LayerNorm.weight'}\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.33, patience=2, verbose=True)\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, class_mapping):\n",
    "    \n",
    "    y_pred_class = torch.argmax(y_pred, dim=-1)\n",
    "    reverse_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "\n",
    "    metrics = []\n",
    "    for i in reverse_class_mapping:\n",
    "\n",
    "        true_positives = torch.sum((y_pred_class == i) & (y_true == i)).item()\n",
    "        true_negatives = torch.sum((y_pred_class != i) & (y_true != i)).item()\n",
    "        false_positives = torch.sum((y_pred_class == i) & (y_true != i)).item()\n",
    "        false_negatives = torch.sum((y_pred_class != i) & (y_true == i)).item()\n",
    "\n",
    "        accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "\n",
    "        class_name = reverse_class_mapping[i]\n",
    "        metrics.append(f'Accuracy_{class_name}: {accuracy:.4f} | Precision_{class_name}: {precision:.4f} | Recall_{class_name}: {recall:.4f}|')\n",
    "\n",
    "    metrics = \" \".join(metrics)\n",
    "    return metrics\n",
    "best_val_loss = 10000\n",
    "best_val_epoch = -1\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_scope3_tensor = torch.tensor([]).to(device)\n",
    "    running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        \n",
    "        input_id_tensors = data[0].to(device)\n",
    "        input_mask_tensors = data[1].to(device)\n",
    "        scope3_tensors = data[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "        scope3_loss = ce_loss(outputs, scope3_tensors)\n",
    "\n",
    "        final_loss = scope3_loss\n",
    "\n",
    "        total_train_loss += final_loss.item()\n",
    "\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "        running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs])\n",
    "\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        average_train_loss = total_train_loss / (i+1)\n",
    "    \n",
    "        print(f'\\rBatch [{i+1}/{num_batches}], Average Train Loss: {average_train_loss:.4f}', scope3_metrics, end='')\n",
    "\n",
    "    print(\"\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        total_valid_loss = 0\n",
    "        running_scope3_tensor = torch.tensor([]).to(device)\n",
    "        running_scope3_pred_tensor = torch.tensor([]).to(device)\n",
    "\n",
    "        for data in valid_dataloader:\n",
    "            \n",
    "            input_id_tensors = data[0].to(device)\n",
    "            input_mask_tensors = data[1].to(device)\n",
    "            scope3_tensors = data[2].to(device)\n",
    "\n",
    "            outputs = model(input_id_tensors, mask=input_mask_tensors)\n",
    "\n",
    "            scope3_loss = ce_loss(outputs, scope3_tensors)\n",
    "\n",
    "            final_loss = scope3_loss\n",
    "            \n",
    "            total_valid_loss += final_loss.item()\n",
    "\n",
    "            running_scope3_tensor = torch.cat([running_scope3_tensor, scope3_tensors])\n",
    "            running_scope3_pred_tensor = torch.cat([running_scope3_pred_tensor, outputs])\n",
    "        \n",
    "        average_valid_loss = total_valid_loss / len(valid_dataloader)\n",
    "        scheduler.step(average_valid_loss)\n",
    "\n",
    "        if average_valid_loss < best_val_loss:\n",
    "            best_val_loss = average_valid_loss\n",
    "            best_val_epoch = epoch_i\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        scope3_metrics = calculate_metrics(running_scope3_pred_tensor, running_scope3_tensor, scope3_class_mapping)\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Avg Validation Loss: {average_valid_loss:.4f} | Best Validation Loss: {best_val_loss:.4f} | Best Epoch: {best_val_epoch}',scope3_metrics)\n",
    "        \n",
    "print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
